{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14619981-9fc5-4f3c-b408-8aa4f8790ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import computational packages\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sparse matrix\n",
    "from scipy.io import mmread\n",
    "\n",
    "# import modeling packages\n",
    "from scipy.stats import chi2, nbinom\n",
    "from scipy.optimize import minimize, dual_annealing\n",
    "\n",
    "# parallelizing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fbfdde-f591-48e9-9e64-dfaf8268cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_and_output_vectors(spacer_sequence, proximal_genes):\n",
    "    '''helper function to get input and output vectors'''\n",
    "    \n",
    "    # get sample of data for cells with guide\n",
    "    guide_present = cell_ts_matrix.loc[spacer_sequence] == 1\n",
    "    guide_cells = cell_ts_matrix.loc[spacer_sequence][guide_present]\n",
    "    guide_cells = guide_cells.sample(min(len(guide_cells), 1000))\n",
    "    \n",
    "    # get sample of data for cells without guide\n",
    "    no_guide_cells = cell_ts_matrix.loc[spacer_sequence][~guide_present]\n",
    "    no_guide_cells = no_guide_cells.sample(min(len(no_guide_cells), 1000))\n",
    "    \n",
    "    # get merged dataframe with sampled input and output\n",
    "    input_series = pd.concat([guide_cells, no_guide_cells])\n",
    "    output_series = count_matrix_df.loc[proximal_genes]\n",
    "    output_series = output_series[output_series.index.isin(input_series.index)]\n",
    "    input_output_df = pd.merge(input_series, output_series, left_index=True, right_index=True)\n",
    "    \n",
    "    # get input and output vectors as np.arrays (for faster computation)\n",
    "    input_vector = input_series.values\n",
    "    output_vector = output_series.values\n",
    "    \n",
    "    return input_vector, output_vector, input_output_df.index\n",
    "\n",
    "\n",
    "def adjust_index(covariate, idx):\n",
    "    '''adjusts index of covariate to match input and output vectors'''\n",
    "    \n",
    "    index_df = pd.DataFrame(idx).set_index(0)\n",
    "    covariate_df = index_df.merge(covariate, left_index=True, right_index=True)\n",
    "    return covariate_df.values\n",
    "\n",
    "\n",
    "def divide_by_guide(input_vector, divide_vector):\n",
    "    '''divide a given divide vector based on an input vector'''\n",
    "    \n",
    "    # split divide vector based on guide presence\n",
    "    cells_with_guide = divide_vector[input_vector.astype(bool)]\n",
    "    cells_wo_guide = divide_vector[~input_vector.astype(bool)]\n",
    "    \n",
    "    return cells_with_guide, cells_wo_guide\n",
    "\n",
    "\n",
    "def nbinom_ll(params, x, counts, s): # , s_scores, g2m_scores,\n",
    "              # cell_guide_counts, percent_mito, prep_batch_1, prep_batch_2):\n",
    "    '''\n",
    "    this function calculates the log-likelihood for the alternative hypothesis\n",
    "    alternative hypothesis - beta1 != 0 \n",
    "    params[0] = beta0 (intercept term)\n",
    "    params[1] = beta1 (guide effect size)\n",
    "    params[2] = sqrt(disperion)\n",
    "    params[3] = beta2 for cell cycle s phase score\n",
    "    params[4] = beta3 for cell cycle g2m phase score\n",
    "    params[5] = beta4 for cell gRNA counts\n",
    "    params[6] = beta5 for percent mitochondrial transcripts\n",
    "    params[7] = beta6 for prep batch 1\n",
    "    params[8] = beta7 for prep batch 2\n",
    "    x1 = indicator vector\n",
    "    counts = vector of counts for UMIs of a gene in each cell\n",
    "    s = scaling factor\n",
    "    s_scores = S phase scores for each cell\n",
    "    g2m_scores = G2 and M phase scores for each cell \n",
    "    cell_guide_counts = gRNA counts for each cell\n",
    "    percent_mito = percentage mitochondrial transcripts for each cell \n",
    "    prep_batch_1 = indicator vector for prep batch 1 (by cell)\n",
    "    prep_batch_2 = indicator vector for prep batch 2 (by cell)\n",
    "    '''\n",
    "    \n",
    "    # calculate value for dispersion of negative binomial distribution\n",
    "    disp = np.exp(params[2])\n",
    "    \n",
    "    # calculate mu for negative binomial distribution\n",
    "    mu = np.exp(params[0] + x * params[1] # + params[3] * s_scores + \n",
    "                # params[4] * g2m_scores + params[5] * cell_guide_counts + \n",
    "                # params[6] * percent_mito + # params[7] * prep_batch_1 + \n",
    "                # params[8] * prep_batch_2 \n",
    "                + np.log(s))\n",
    "    \n",
    "    # calculate probability value for negative binomial distribution\n",
    "    prob = disp / (disp + mu)\n",
    "    \n",
    "    # calculate log-likelihood vector for observed counts\n",
    "    ll = nbinom.logpmf(counts, n=disp, p=prob)\n",
    "    \n",
    "    # take sum of log likelihood vector\n",
    "    return -ll.sum()\n",
    "\n",
    "\n",
    "def null_ll(params, counts, s): #  s_scores, g2m_scores, \n",
    "            # cell_guide_counts, percent_mito, prep_batch_1, prep_batch_2):\n",
    "    '''\n",
    "    this function calculates the log-likelihood for the null hypothesis\n",
    "    null hypothesis - beta1 == 0 \n",
    "    params[0] = beta0 (intercept term)\n",
    "    params[1] = sqrt(dispersion)\n",
    "    params[2] = beta2 for cell cycle s phase score\n",
    "    params[3] = beta3 for cell cycle g2m phase score\n",
    "    params[4] = beta4 for cell gRNA counts\n",
    "    params[5] = beta5 for percent mitochondrial transcripts\n",
    "    params[6] = beta6 for prep batch 1\n",
    "    params[7] = beta7 for prep batch 2\n",
    "    counts = vector of counts for UMIs of a gene in each cell\n",
    "    s = scaling factor\n",
    "    s_scores = S phase scores for each cell\n",
    "    g2m_scores = G2 and M phase scores for each cell \n",
    "    cell_guide_counts = gRNA counts for each cell\n",
    "    percent_mito = percentage mitochondrial transcripts for each cell \n",
    "    prep_batch_1 = indicator vector for prep batch 1 (by cell)\n",
    "    prep_batch_2 = indicator vector for prep batch 2 (by cell)\n",
    "    '''\n",
    "    \n",
    "    # calculate value for dispersion of negative binomial distribution\n",
    "    disp = np.exp(params[1])\n",
    "    \n",
    "    # calculate mu for negative binomial distribution \n",
    "    mu = np.exp(params[0] + # params[2] * s_scores + params[3] * g2m_scores + \n",
    "                # params[4] * cell_guide_counts + params[5] * percent_mito + \n",
    "                # params[6] * prep_batch_1 + params[7] * prep_batch_2 + \n",
    "                np.log(s))\n",
    "    \n",
    "    # calculate probability value for negative binomial distribution\n",
    "    prob = disp / (disp + mu)\n",
    "    \n",
    "    # calculate log-likelihood vector for observed counts\n",
    "    ll = nbinom.logpmf(counts, n=disp, p=prob)\n",
    "    \n",
    "    # take sum of log-likelihood vector\n",
    "    return -ll.sum()\n",
    "\n",
    "\n",
    "def write_success_output(spacer_sequence, proximal_genes, null, alt):\n",
    "    '''write model information if both optimizations successful'''\n",
    "    \n",
    "    print(spacer_sequence)\n",
    "    print(proximal_genes)\n",
    "    \n",
    "    # get null GLM information as a list\n",
    "    null_coeffs = [str(x) for x in null.x]\n",
    "    print(len(null_coeffs))\n",
    "    print(null_coeffs)\n",
    "    print('null beta 0: ' + null_coeffs[0])\n",
    "    print('null dispersion: ' + null_coeffs[1])\n",
    "#     print('null s score beta: ' + null_coeffs[2])\n",
    "#     print('null g2m score beta: ' + null_coeffs[3])\n",
    "#     print('null cell guide counts beta: ' + null_coeffs[4])\n",
    "#     print('null percent mito beta: ' + null_coeffs[5])\n",
    "#     print('null prep batch 1 beta: ' + null_coeffs[6])\n",
    "#     print('null prep batch 2 beta: ' + null_coeffs[7])\n",
    "    null_function = str(null.fun)\n",
    "    print('null likelihood: ' + null_function)\n",
    "    \n",
    "    # get alternative GLM information as a list\n",
    "    alt_coeffs = [str(x) for x in alt.x]\n",
    "    print(len(alt_coeffs))\n",
    "    print(alt_coeffs)\n",
    "    print('alt beta 0: ' + alt_coeffs[0])\n",
    "    print('alt beta 1: ' + alt_coeffs[1])\n",
    "    print('alt dispersion: ' + alt_coeffs[2])\n",
    "#     print('alt s score beta: ' + alt_coeffs[3])\n",
    "#     print('alt g2m score beta: ' + alt_coeffs[4])\n",
    "#     print('alt cell guide counts beta: ' + alt_coeffs[5])\n",
    "#     print('alt percent mito beta: ' + alt_coeffs[6])\n",
    "#     print('alt prep batch 1 beta: ' + alt_coeffs[7])\n",
    "#     print('alt prep batch 2 beta: ' + alt_coeffs[8])\n",
    "    alt_function = str(alt.fun)\n",
    "    print('alt likelihood: ' + alt_function)\n",
    "    \n",
    "    # create list of output information and convert to comma separated string\n",
    "    output_list = [proximal_genes, spacer_sequence] + null_coeffs + \\\n",
    "                  [null_function] + alt_coeffs + [alt_function]\n",
    "    output_string = ','.join(output_list)\n",
    "    # print(output_string)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def write_error_output(spacer_sequence, proximal_genes):\n",
    "    '''write guide and gene information if one or both optimizations failed'''\n",
    "    \n",
    "    # create error string and write to error file \n",
    "    error_list = [proximal_genes, spacer_sequence, 'optimization failed']\n",
    "    error_string = ','.join(error_list)\n",
    "    print(error_string)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def run_likelihood_ratio_test(null_function, alt_function):\n",
    "    '''helper function to run likelihood test'''\n",
    "    \n",
    "    # negate function outputs to get null and alternative likelihood\n",
    "    null_likelihood = -1 * null_function\n",
    "    alt_likelihood = -1 * alt_function\n",
    "    \n",
    "    # calculate test statistic and p-value\n",
    "    ts = -2 * (null_likelihood - alt_likelihood)\n",
    "    log_pval = chi2.logsf(ts, 1)\n",
    "    pval = np.exp(log_pval)\n",
    "    \n",
    "    # return p-value\n",
    "    return pval\n",
    "\n",
    "\n",
    "def print_coeffs(x):\n",
    "    print(x)\n",
    "\n",
    "\n",
    "def run_glm_optimizer(spacer_sequence, proximal_genes, max_iters):\n",
    "    '''function to fit null and alternative models for given guide-gene pair'''\n",
    "\n",
    "    # get input and output vectors\n",
    "    input_vector, output_vector, idx = get_input_and_output_vectors(spacer_sequence, \n",
    "                                                                    proximal_genes)\n",
    "#     print(len(input_vector))\n",
    "#     print(len(output_vector))\n",
    "    \n",
    "    # adjust covariates and scalers with new index\n",
    "    scalers_arr = adjust_index(scaling_factors, idx)\n",
    "#     cell_s_arr = adjust_index(cell_s_scores, idx)\n",
    "#     cell_g2m_arr = adjust_index(cell_g2m_scores, idx)\n",
    "#     cell_guide_arr = adjust_index(cell_guide_counts, idx)\n",
    "#     percent_mito_arr = adjust_index(percent_mito, idx)\n",
    "#     prep_batch_1_arr = adjust_index(prep_batch_1, idx)\n",
    "#     prep_batch_2_arr = adjust_index(prep_batch_2, idx)\n",
    "\n",
    "    # get subsets of output vector based on guide presence in cells\n",
    "    cells_with_guide, cells_wo_guide = divide_by_guide(input_vector, \n",
    "                                                       output_vector)\n",
    "    \n",
    "    # get subsets of scaling factors based on guide presence in cells\n",
    "    scalers_with_guide, scalers_wo_guide = divide_by_guide(input_vector, \n",
    "                                                            scalers_arr)\n",
    "\n",
    "    # calculate initial parameter estimates\n",
    "    beta0_estimate = np.mean(np.log(cells_wo_guide + 1) - \\\n",
    "                             np.log(scalers_wo_guide))\n",
    "    # beta0_estimate = 1\n",
    "#     beta0_estimate = np.random.random() * 20\n",
    "    beta1_estimate = beta0_estimate - np.mean(np.log(cells_with_guide + 1) - \\\n",
    "                                             np.log(scalers_with_guide))\n",
    "    disp_estimate = -1\n",
    "    \n",
    "#     print(beta0_estimate)\n",
    "#     print(disp_estimate)\n",
    "    \n",
    "    model_diff = np.Inf\n",
    "    null_estimates = (beta0_estimate, disp_estimate)\n",
    "    \n",
    "    # fit null model using scipy optimizer\n",
    "    null = minimize(null_ll, \n",
    "                    x0=null_estimates, # 0, 0, 0, 0, 0, 0), \n",
    "                    args=(output_vector, scalers_arr), \n",
    "                          # cell_s_arr, cell_g2m_arr,\n",
    "                          # cell_guide_arr, percent_mito_arr,\n",
    "                          # prep_batch_1_arr, prep_batch_2_arr),\n",
    "                    method='Nelder-Mead', \n",
    "                    options={'maxiter': max_iters})\n",
    "                    # callback=print_coeffs) # 'maxfev': max_iters,\n",
    "                             # 'xatol': 1e-5, 'fatol': 1e-5})\n",
    "        \n",
    "    alt_estimates = (beta0_estimate, beta1_estimate, disp_estimate)\n",
    "    \n",
    "    # fit alternative model using scipy optimizer\n",
    "    alt = minimize(nbinom_ll, \n",
    "                   x0=alt_estimates, # 0, 0, 0, 0, 0, 0), \n",
    "                   args=(input_vector, output_vector, \n",
    "                         scalers_arr), # , cell_s_arr, cell_g2m_arr,\n",
    "                         # cell_guide_arr, percent_mito_arr,\n",
    "                         # prep_batch_1_arr, prep_batch_2_arr),\n",
    "                   method='Nelder-Mead', \n",
    "                   options={'maxiter': max_iters})\n",
    "                   # callback=print_coeffs) # 'maxfev': max_iters,\n",
    "                            # 'xatol': 1e-5, 'fatol': 1e-5})\n",
    "    \n",
    "#     while model_diff > 70:\n",
    "\n",
    "#         # fit null model using scipy optimizer\n",
    "#         null = minimize(null_ll, \n",
    "#                         x0=null_estimates, # 0, 0, 0, 0, 0, 0), \n",
    "#                         args=(output_vector, scalers_arr), \n",
    "#                               # cell_s_arr, cell_g2m_arr,\n",
    "#                               # cell_guide_arr, percent_mito_arr,\n",
    "#                               # prep_batch_1_arr, prep_batch_2_arr),\n",
    "#                         method='Nelder-Mead', \n",
    "#                         options={'maxiter': max_iters},\n",
    "#                         callback=print_coeffs) # 'maxfev': max_iters,\n",
    "#                                  # 'xatol': 1e-5, 'fatol': 1e-5})\n",
    "\n",
    "#         # fit alternative model using scipy optimizer\n",
    "#         alt = minimize(nbinom_ll, \n",
    "#                        x0=alt_estimates, # 0, 0, 0, 0, 0, 0), \n",
    "#                        args=(input_vector, output_vector, \n",
    "#                              scalers_arr), # , cell_s_arr, cell_g2m_arr,\n",
    "#                              # cell_guide_arr, percent_mito_arr,\n",
    "#                              # prep_batch_1_arr, prep_batch_2_arr),\n",
    "#                        method='Nelder-Mead', \n",
    "#                        options={'maxiter': max_iters},\n",
    "#                        callback=print_coeffs) # 'maxfev': max_iters,\n",
    "#                                 # 'xatol': 1e-5, 'fatol': 1e-5})\n",
    "        \n",
    "#         model_diff = np.abs(null.fun - alt.fun)\n",
    "#         print(model_diff)\n",
    "        \n",
    "#         if null.fun < alt.fun:\n",
    "#             null_estimates = (null.x[0], null.x[1])\n",
    "#             alt_estimates = (null.x[0], 0, null.x[1])\n",
    "        \n",
    "#         if alt.fun < null.fun:\n",
    "#             null_estimates = (alt.x[0], alt.x[2])\n",
    "#             alt_estimates = (alt.x[0], alt.x[1], alt.x[2])\n",
    "\n",
    "#     null_bounds = ((0, 20), (0, 5), (-2, 2), (-2, 2), (-2, 2), (-2, 2), (-2, 2), (-2, 2))\n",
    "#     alt_bounds = ((0, 20), (-2, 2), (0, 5), (-2, 2), (-2, 2), (-2, 2), (-2, 2), (-2, 2), (-2, 2))\n",
    "#     null = dual_annealing(null_ll, x0=(beta0_estimate, disp_estimate, 0, 0, 0, 0, 0, 0),\n",
    "#                           bounds=null_bounds,\n",
    "#                           args=(output_vector, scalers_arr, cell_s_arr, cell_g2m_arr, \n",
    "#                                 cell_guide_arr, percent_mito_arr, prep_batch_1_arr,\n",
    "#                                 prep_batch_2_arr),\n",
    "#                           maxiter=max_iters)\n",
    "    \n",
    "#     alt = dual_annealing(nbinom_ll, x0=(beta0_estimate, 0, disp_estimate, 0, 0, 0, 0, 0, 0),\n",
    "#                          bounds=alt_bounds,\n",
    "#                          args=(output_vector, scalers_arr, cell_s_arr, cell_g2m_arr, \n",
    "#                                cell_guide_arr, percent_mito_arr, prep_batch_1_arr,\n",
    "#                                prep_batch_2_arr),\n",
    "#                          maxiter=max_iters) \n",
    "    \n",
    "    # if both optimizations run successfully\n",
    "    if null.success and alt.success:\n",
    "        \n",
    "        # print successful output\n",
    "        print('successful output!')\n",
    "        \n",
    "        # write model information to results file\n",
    "        write_success_output(spacer_sequence, proximal_genes, null, alt)\n",
    "        \n",
    "        fold_change_expression = np.exp(alt.x[0] + alt.x[1]) / np.exp(null.x[0])\n",
    "        print('fold change expression: ' + str(fold_change_expression))\n",
    "        \n",
    "        # perform likelihood ratio test and return p-value\n",
    "        # pval = run_likelihood_ratio_test(null.fun, alt.fun)\n",
    "        pval = run_likelihood_ratio_test(null.fun, alt.fun)\n",
    "        \n",
    "#         null_beta0 = null.x[0]\n",
    "#         null_disp = null.x[1]\n",
    "        \n",
    "#         alt_beta0 = alt.x[0]\n",
    "#         alt_beta1 = alt.x[1]\n",
    "#         alt_disp = alt.x[2]\n",
    "        \n",
    "# #         print(alt_beta0)\n",
    "# #         print(alt_beta1)\n",
    "# #         print(alt_disp)\n",
    "        \n",
    "#         disp_values = np.arange(-1.5, 1, 0.1)\n",
    "#         beta1_values = np.arange(-1, 1, 0.5)\n",
    "#         beta0_values = np.arange(8.8, 9.3, 0.05)\n",
    "        \n",
    "#         null_lls = []\n",
    "#         alt_lls = []\n",
    "        \n",
    "#         ll_values = [0] * len(disp_values)\n",
    "#         counter = 0\n",
    "        \n",
    "#         for disp in disp_values:\n",
    "#             null_lls.append(null_ll_disp(null_beta0, disp, output_vector, scalers_arr))\n",
    "#             alt_lls.append(nbinom_ll_disp(alt_beta0, alt_beta1, disp, \n",
    "#                                           output_vector, scalers_arr, input_vector))\n",
    "            \n",
    "#         for disp in disp_values:\n",
    "#             disp_lls = []\n",
    "#             for beta0 in beta0_values:\n",
    "#                 disp_lls.append(null_ll_disp(beta0, disp, output_vector, scalers_arr))\n",
    "#             ll_values[counter] = disp_lls\n",
    "#             counter += 1\n",
    "            \n",
    "#             alt_lls.append(nbinom_ll_disp(alt_beta0, alt_beta1, value, \n",
    "#                                           output_vector, scalers_arr, input_vector))\n",
    "#         fig = plt.figure()\n",
    "#         ax = plt.gca()\n",
    "#         # print(ll_values)\n",
    "#         plt.contourf(beta0_values, disp_values, ll_values)\n",
    "#         fig, axs = plt.subplots(2)\n",
    "#         axs[0].plot(disp_values, null_lls)\n",
    "#         axs[1].plot(disp_values, alt_lls)\n",
    "        \n",
    "        return pval\n",
    "    \n",
    "    \n",
    "    # if one or both of the optimizations fail\n",
    "    else:\n",
    "        \n",
    "        # print error output\n",
    "        print('error output')\n",
    "        print(null.success)\n",
    "        print(alt.success)\n",
    "        \n",
    "        # write guide-gene info to error file\n",
    "        write_error_output(spacer_sequence, proximal_genes)\n",
    "        \n",
    "        # return np.nan as a placeholder\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689deef8-ca74-4ce4-84ce-a156464849d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in cell-guide matrix...\n",
      "reading in scaling factors...\n",
      "creating scaling factors array (for computation)...\n",
      "reading in count matrix...\n"
     ]
    }
   ],
   "source": [
    "# set path to project directory \n",
    "project_path = '/iblm/netapp/home/karthik/gasperini_project/'\n",
    "data_path = '/iblm/netapp/data1/external/Gasperini2019/'\n",
    "\n",
    "# read in cell-guide matrix\n",
    "print('loading in cell-guide matrix...')\n",
    "cell_ts_matrix = pd.read_hdf(project_path + 'data/cell_ts_matrix.h5')\n",
    "cell_ts_matrix.index = cell_ts_matrix.index.str.lower()\n",
    "\n",
    "# read in scaling factors\n",
    "print('reading in scaling factors...')\n",
    "scaling_factors = pd.read_csv(project_path + 'data/scaling_factors.csv')\n",
    "scaling_factors.columns = ['cell', 'scaling_factor']\n",
    "scaling_factors = scaling_factors.set_index('cell')\n",
    "\n",
    "# filter scaling factors to only include cells in cell-guide matrix\n",
    "print('creating scaling factors array (for computation)...')\n",
    "scaling_factors = scaling_factors.merge(cell_ts_matrix.iloc[0], on='cell')\n",
    "scaling_factors = scaling_factors.iloc[:, 0]\n",
    "scaling_factors_arr = scaling_factors.values # np.array for faster computation\n",
    "scalers_arr = scaling_factors_arr\n",
    "\n",
    "# read in UMI count matrix\n",
    "print('reading in count matrix...')\n",
    "count_matrix = mmread(data_path + 'suppl/GSE120861_at_scale_screen.exprs.mtx')\n",
    "count_matrix_df = pd.DataFrame(count_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306bd369-eac8-4452-9b1f-17c185fb3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in column names from corresponding cells file\n",
    "colnames = []\n",
    "with open(data_path + 'suppl/GSE120861_at_scale_screen.cells.txt') as f:\n",
    "    colnames = f.readlines()\n",
    "colnames = pd.Series(colnames).str.strip()\n",
    "\n",
    "# read in index (row names) from corresponding genes file \n",
    "rownames = []\n",
    "with open(data_path + 'suppl/GSE120861_at_scale_screen.genes.txt') as f:\n",
    "    rownames = f.readlines()\n",
    "rownames = pd.Series(rownames).str.strip()\n",
    "\n",
    "# assign row and column names to UMI count matrix\n",
    "count_matrix_df.index = rownames\n",
    "count_matrix_df.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39036cf6-8dc1-4664-a3db-2c7f2798954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for top half of cells\n",
    "scaling_factors = scaling_factors.sort_values(ascending=False).iloc[0:len(scaling_factors) // 2]\n",
    "scaling_factors_arr = scaling_factors.values\n",
    "scalers_arr = scaling_factors_arr\n",
    "\n",
    "count_matrix_df = (count_matrix_df.T[count_matrix_df.columns.isin(scaling_factors.index)]).T\n",
    "cell_ts_matrix = (cell_ts_matrix.T[cell_ts_matrix.columns.isin(scaling_factors.index)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0b26ea-8158-4e0e-960f-17619859005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in cell cycle scores...\n"
     ]
    }
   ],
   "source": [
    "# read in cell cycle scores and merge into single dataframe\n",
    "print('reading in cell cycle scores...')\n",
    "s_scores = pd.read_csv(project_path + 'data/s_scores.csv', index_col=0)\n",
    "g2m_scores = pd.read_csv(project_path + 'data/g2m_scores.csv', index_col=0)\n",
    "s_scores.index = s_scores.index.rename('cell')\n",
    "g2m_scores.index = g2m_scores.index.rename('cell')\n",
    "merged_scores = pd.merge(s_scores, g2m_scores, on='cell')\n",
    "\n",
    "# filter scores to only include cells in guide matrix\n",
    "in_guide_matrix = merged_scores.index.isin(cell_ts_matrix.columns)\n",
    "merged_scores = merged_scores[in_guide_matrix]\n",
    "cell_s_scores = merged_scores['S.Score']\n",
    "cell_g2m_scores = merged_scores['G2M.Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad69ebb2-8a21-43fe-9f97-cd46f0cb3209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding Gasperini paper covariates\n"
     ]
    }
   ],
   "source": [
    "# read in phenodata file to add additional covariates\n",
    "print('adding Gasperini paper covariates')\n",
    "colnames = open(data_path + 'suppl/GSE120861_at_scale.phenoData.colnames.txt')\\\n",
    "           .read().splitlines()\n",
    "phenodata_df = pd.read_csv(project_path + 'data/phenodata.txt', sep=' ', \n",
    "                           names=colnames)\n",
    "phenodata_df = phenodata_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# get guide count, mitochondrial transcripts, and prep batch as covariate\n",
    "cell_guide_counts = phenodata_df['guide_count']\n",
    "percent_mito = phenodata_df['percent.mito']\n",
    "prep_batch_1 = (phenodata_df['prep_batch'] == 'prep_batch_1').astype(np.int64)\n",
    "prep_batch_2 = (phenodata_df['prep_batch'] == 'prep_batch_2').astype(np.int64)\n",
    "\n",
    "# cell_guide_counts.index = cell_ts_matrix.columns\n",
    "# percent_mito.index = cell_ts_matrix.columns\n",
    "# prep_batch_1.index = cell_ts_matrix.columns\n",
    "# prep_batch_2.index = cell_ts_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21cbaecb-8499-4837-9c4e-5cfa34f17eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in guide-gene pairs...\n"
     ]
    }
   ],
   "source": [
    "# read in guide-gene pairs data frame and drop genes not in genes data frame\n",
    "print('reading in guide-gene pairs...')\n",
    "guide_gene_pairs = pd.read_csv(project_path + \n",
    "                               'data/high_confidence_ts_genes.csv')\n",
    "guide_gene_pairs = guide_gene_pairs[guide_gene_pairs['high_confidence_subset']]\n",
    "guide_gene_pairs = guide_gene_pairs[['Target_Site', 'ENSG']]\n",
    "guide_gene_pairs.columns = ['target_site', 'gene']\n",
    "guide_gene_pairs['target_site'] = guide_gene_pairs['target_site'].apply(lambda x: x + '_top_two').str.lower()\n",
    "in_genes_df = guide_gene_pairs['gene'].isin(count_matrix_df.index)\n",
    "guide_gene_pairs = guide_gene_pairs[in_genes_df].reset_index(drop=True)\n",
    "guide_gene_pairs['target_site'] = guide_gene_pairs['target_site'].str.lower()\n",
    "in_cell_guide_mtx = guide_gene_pairs['target_site'].isin(cell_ts_matrix.index)\n",
    "guide_gene_pairs = guide_gene_pairs[in_cell_guide_mtx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a6ddf9-acdd-4ec8-8676-2ff29247a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write guide gene pairs to CSV file \n",
    "guide_gene_pairs.to_csv(project_path + 'results/high_confidence_validations.csv', \n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c2310c8-af36-4012-8f2b-d57b069c4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_gene_pairs = guide_gene_pairs.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d7345e-792b-434b-8ff5-8272eb804710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format np array to run optimizer\n",
    "optimizer_df = guide_gene_pairs[['target_site', 'gene']]\n",
    "optimizer_df = optimizer_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e01dc2b0-2ed9-4b7d-8861-dffb4e298374",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_optimizer_df = []\n",
    "for lst in optimizer_df:\n",
    "    iter_optimizer_df.append(np.append(lst, 5000))\n",
    "iter_optimizer_df = np.array(iter_optimizer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e917cc46-b642-4752-8497-09645c25907f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iter_optimizer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeabf98a-3585-40f0-ad2b-0174018f7de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['chr7.3907_top_two', 'ENSG00000189056', 5000],\n",
       "       ['chr10.143_top_two', 'ENSG00000067082', 5000],\n",
       "       ['chr1.10033_top_two', 'ENSG00000116199', 5000],\n",
       "       ['chr15.1149_top_two', 'ENSG00000128872', 5000],\n",
       "       ['chr4.1828_top_two', 'ENSG00000124882', 5000],\n",
       "       ['chr1.7885_top_two', 'ENSG00000081026', 5000],\n",
       "       ['chr4.2004_top_two', 'ENSG00000163297', 5000],\n",
       "       ['chr9.120_top_two', 'ENSG00000099219', 5000],\n",
       "       ['chr22.1058_top_two', 'ENSG00000128294', 5000],\n",
       "       ['chr10.3436_top_two', 'ENSG00000107438', 5000]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_optimizer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4612df1d-ce17-4a4f-8fee-dfca1a711832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful output!\n",
      "chr22.1058_top_two\n",
      "ENSG00000128294\n",
      "2\n",
      "['12.144379253122995', '0.8494520289102145']\n",
      "null beta 0: 12.144379253122995\n",
      "null dispersion: 0.8494520289102145\n",
      "null likelihood: 2328741.625218029\n",
      "3\n",
      "['12.153641136914668', '-0.048694098143447814', '0.8504640355341097']\n",
      "alt beta 0: 12.153641136914668\n",
      "alt beta 1: -0.048694098143447814\n",
      "alt dispersion: 0.8504640355341097\n",
      "alt likelihood: 2328520.6405782476\n",
      "fold change expression: 0.9613351165060653\n",
      "successful output!\n",
      "chr7.3907_top_two\n",
      "ENSG00000189056\n",
      "2\n",
      "['10.865598069614034', '-0.26757256349639125']\n",
      "null beta 0: 10.865598069614034\n",
      "null dispersion: -0.26757256349639125\n",
      "null likelihood: 1339088.541234595\n",
      "3\n",
      "['10.93276674121881', '-0.31126239912510395', '-0.2447907835654881']\n",
      "alt beta 0: 10.93276674121881\n",
      "alt beta 1: -0.31126239912510395\n",
      "alt dispersion: -0.2447907835654881\n",
      "alt likelihood: 1335789.9320308312\n",
      "fold change expression: 0.7834142033509979\n",
      "successful output!\n",
      "chr9.120_top_two\n",
      "ENSG00000099219\n",
      "2\n",
      "['9.221786936550558', '-0.798925770138213']\n",
      "null beta 0: 9.221786936550558\n",
      "null dispersion: -0.798925770138213\n",
      "null likelihood: 560858.3246095633\n",
      "3\n",
      "['9.293895216131133', '-0.23712315229693998', '-0.7837269254833665']\n",
      "alt beta 0: 9.293895216131133\n",
      "alt beta 1: -0.23712315229693998\n",
      "alt dispersion: -0.7837269254833665\n",
      "alt likelihood: 560100.3344084808\n",
      "fold change expression: 0.8478810936991235\n",
      "successful output!\n",
      "chr1.10033_top_two\n",
      "ENSG00000116199\n",
      "2\n",
      "['11.522572046424191', '0.8893683410257491']\n",
      "null beta 0: 11.522572046424191\n",
      "null dispersion: 0.8893683410257491\n",
      "null likelihood: 2517013.422465898\n",
      "3\n",
      "['11.499765975342505', '0.06647196009718093', '0.8924808753957147']\n",
      "alt beta 0: 11.499765975342505\n",
      "alt beta 1: 0.06647196009718093\n",
      "alt dispersion: 0.8924808753957147\n",
      "alt likelihood: 2516437.650196157\n",
      "fold change expression: 1.0446332731243346\n",
      "successful output!\n",
      "chr4.1828_top_two\n",
      "ENSG00000124882\n",
      "2\n",
      "['10.663731823754274', '-0.8750112381678581']\n",
      "null beta 0: 10.663731823754274\n",
      "null dispersion: -0.8750112381678581\n",
      "null likelihood: 1312703.5439004423\n",
      "3\n",
      "['10.724389736900832', '-0.22813663874534285', '-0.8646943770232725']\n",
      "alt beta 0: 10.724389736900832\n",
      "alt beta 1: -0.22813663874534285\n",
      "alt dispersion: -0.8646943770232725\n",
      "alt likelihood: 1311063.0526989412\n",
      "fold change expression: 0.8457946108713402\n",
      "successful output!\n",
      "chr15.1149_top_two\n",
      "ENSG00000128872\n",
      "2\n",
      "['9.582032920672672', '1.0932788031911675']\n",
      "null beta 0: 9.582032920672672\n",
      "null dispersion: 1.0932788031911675\n",
      "null likelihood: 832851.2820501644\n",
      "3\n",
      "['9.612778165830239', '-0.08535468296842352', '1.1000758525932701']\n",
      "alt beta 0: 9.612778165830239\n",
      "alt beta 1: -0.08535468296842352\n",
      "alt dispersion: 1.1000758525932701\n",
      "alt likelihood: 832653.5735133018\n",
      "fold change expression: 0.9468548814621243\n",
      "successful output!\n",
      "chr4.2004_top_two\n",
      "ENSG00000163297\n",
      "2\n",
      "['11.196486348429186', '-0.3274653254169366']\n",
      "null beta 0: 11.196486348429186\n",
      "null dispersion: -0.3274653254169366\n",
      "null likelihood: 2199226.00349366\n",
      "3\n",
      "['11.129147540002814', '0.1813449407961848', '-0.3174972154877161']\n",
      "alt beta 0: 11.129147540002814\n",
      "alt beta 1: 0.1813449407961848\n",
      "alt dispersion: -0.3174972154877161\n",
      "alt likelihood: 2196677.5591314198\n",
      "fold change expression: 1.1207589977717247\n",
      "successful output!\n",
      "chr10.3436_top_two\n",
      "ENSG00000107438\n",
      "2\n",
      "['12.083163004446686', '0.9719692705952234']\n",
      "null beta 0: 12.083163004446686\n",
      "null dispersion: 0.9719692705952234\n",
      "null likelihood: 2616155.0802232902\n",
      "3\n",
      "['12.099337328524577', '-0.06710604662427622', '0.9744419148684033']\n",
      "alt beta 0: 12.099337328524577\n",
      "alt beta 1: -0.06710604662427622\n",
      "alt dispersion: 0.9744419148684033\n",
      "alt likelihood: 2615576.5695546926\n",
      "fold change expression: 0.9503435553553349\n",
      "successful output!\n",
      "chr10.143_top_two\n",
      "ENSG00000067082\n",
      "2\n",
      "['11.36389919859876', '-0.670640102789286']\n",
      "null beta 0: 11.36389919859876\n",
      "null dispersion: -0.670640102789286\n",
      "null likelihood: 3044622.1627774746\n",
      "3\n",
      "['11.2874436801631', '0.1716479207214695', '-0.6636471588229133']\n",
      "alt beta 0: 11.2874436801631\n",
      "alt beta 1: 0.1716479207214695\n",
      "alt dispersion: -0.6636471588229133\n",
      "alt likelihood: 3041747.045767201\n",
      "fold change expression: 1.0998704523586822\n",
      "successful output!\n",
      "chr1.7885_top_two\n",
      "ENSG00000081026\n",
      "2\n",
      "['10.76653332053779', '0.466959601080468']\n",
      "null beta 0: 10.76653332053779\n",
      "null dispersion: 0.466959601080468\n",
      "null likelihood: 2185641.4721491616\n",
      "3\n",
      "['10.792804260172693', '-0.0633390066932748', '0.46929114213418177']\n",
      "alt beta 0: 10.792804260172693\n",
      "alt beta 1: -0.0633390066932748\n",
      "alt dispersion: 0.46929114213418177\n",
      "alt likelihood: 2185263.354430602\n",
      "fold change expression: 0.9636105429816614\n"
     ]
    }
   ],
   "source": [
    "# create multiprocessing pool of 100 cores and run in parallel\n",
    "pool = Pool(10)\n",
    "p_values = pool.starmap(run_glm_optimizer, iter_optimizer_df)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d1d29f1-a1e5-4a2e-bcd3-a7bdd8a85ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 2.0711129505047859e-252,\n",
       " 5.477650350834048e-88,\n",
       " 0.0,\n",
       " 1.7684812937456084e-166,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0351272918629966e-98,\n",
       " 1.3362992827331772e-253]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ee39d-701c-4053-8692-97b6664d6112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
